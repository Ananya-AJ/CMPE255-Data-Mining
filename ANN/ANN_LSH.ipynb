{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPwffKqIqSZOJTKOCth+sBv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ananya-AJ/CMPE255-Data-Mining/blob/main/ANN/ANN_LSH.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_F9QHAXGe_cR",
        "outputId": "fc3b900f-e34d-485d-da98-aed9640648fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n"
          ]
        }
      ],
      "source": [
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d benhamner/nips-papers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kI6KdOxPfJDZ",
        "outputId": "55f7e6ec-4437-4789-c820-519935e86b5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading nips-papers.zip to /content\n",
            " 86% 121M/141M [00:01<00:00, 102MB/s]\n",
            "100% 141M/141M [00:01<00:00, 109MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip nips-papers.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KvN_omY8htv3",
        "outputId": "cf7f051f-e01b-4331-92b2-4e063d327c05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  nips-papers.zip\n",
            "  inflating: authors.csv             \n",
            "  inflating: database.sqlite         \n",
            "  inflating: paper_authors.csv       \n",
            "  inflating: papers.csv              \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasketch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0z6Rt-dh6B1",
        "outputId": "5b51973a-83fe-4b5e-a9f5-5fbdc3c71842"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting datasketch\n",
            "  Downloading datasketch-1.5.8-py2.py3-none-any.whl (76 kB)\n",
            "\u001b[K     |████████████████████████████████| 76 kB 3.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasketch) (1.7.3)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from datasketch) (1.21.6)\n",
            "Installing collected packages: datasketch\n",
            "Successfully installed datasketch-1.5.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import time\n",
        "from datasketch import MinHash, MinHashLSHForest"
      ],
      "metadata": {
        "id": "sG29SIvzhzhu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##split text into tokens/shingles based on whitespace"
      ],
      "metadata": {
        "id": "N2syoaW-iKQG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(text):\n",
        "    text = re.sub(r'[^\\w\\s]','',text)\n",
        "    tokens = text.lower()\n",
        "    tokens = tokens.split()\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "jlCcxF1oh34C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Number of Permutations\n",
        "permutations = 128\n",
        "\n",
        "#Number of Recommendations to return\n",
        "num_recommendations = 1"
      ],
      "metadata": {
        "id": "w2ha36SliPVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pass in a dataframe with every string you want to query.\n",
        "Preprocess a string of text using our preprocessing step above.\n",
        "Set the number of permutations in your MinHash.\n",
        "MinHash the string on all of your shingles in the string.\n",
        "Store the MinHash of the string.\n",
        "Repeat 2-5 for all strings in your dataframe.\n",
        "Build a forest of all the MinHashed strings.\n",
        "Index your forest to make it searchable."
      ],
      "metadata": {
        "id": "Ver9aZ5Pj4s1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_forest(data, perms):\n",
        "    start_time = time.time()\n",
        "    \n",
        "    minhash = []\n",
        "    \n",
        "    for text in data['text']:\n",
        "        tokens = preprocess(text)\n",
        "        m = MinHash(num_perm=perms)\n",
        "        for s in tokens:\n",
        "            m.update(s.encode('utf8'))\n",
        "        minhash.append(m)\n",
        "        \n",
        "    forest = MinHashLSHForest(num_perm=perms)\n",
        "    \n",
        "    for i,m in enumerate(minhash):\n",
        "        forest.add(i,m)\n",
        "        \n",
        "    forest.index()\n",
        "    \n",
        "    print('It took %s seconds to build forest.' %(time.time()-start_time))\n",
        "    \n",
        "    return forest"
      ],
      "metadata": {
        "id": "cvEMXV37iYLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocess your text into shingles.\n",
        "Set the same number of permutations for your MinHash as was used to build the forest.\n",
        "Create your MinHash on the text using all your shingles.\n",
        "Query the forest with your MinHash and return the number of requested recommendations.\n",
        "Provide the titles of each conference paper recommended.\n"
      ],
      "metadata": {
        "id": "DLa-tCQXj9BD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(text, database, perms, num_results, forest):\n",
        "    start_time = time.time()\n",
        "    \n",
        "    tokens = preprocess(text)\n",
        "    m = MinHash(num_perm=perms)\n",
        "    for s in tokens:\n",
        "        m.update(s.encode('utf8'))\n",
        "        \n",
        "    idx_array = np.array(forest.query(m, num_results))\n",
        "    if len(idx_array) == 0:\n",
        "        return None # if your query is empty, return none\n",
        "    \n",
        "    result = database.iloc[idx_array]['title']\n",
        "    \n",
        "    print('It took %s seconds to query forest.' %(time.time()-start_time))\n",
        "    \n",
        "    return result"
      ],
      "metadata": {
        "id": "pOkFHJFljAmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing on NIPS"
      ],
      "metadata": {
        "id": "wJVmbVHqjIlD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "db = pd.read_csv('papers.csv')\n",
        "db['text'] = db['title'] + ' ' + db['abstract']\n",
        "forest = get_forest(db, permutations)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNql4xQGjHna",
        "outputId": "99e8b8e1-30e6-490d-f8c8-e75c8d9215e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It took 16.60693335533142 seconds to build forest.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_recommendations = 5\n",
        "title = 'Using LSH to build recommendation system'\n",
        "result = predict(title,db, permutations, num_recommendations, forest)\n",
        "print('\\n Top Recommendation(s) is(are) \\n', result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWL6wsaZjLSR",
        "outputId": "f4ec90b0-001b-433e-8bb4-522f6cc7f162"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It took 0.013130664825439453 seconds to query forest.\n",
            "\n",
            " Top Recommendation(s) is(are) \n",
            " 2592     Random Sampling of States in Dynamic Programming\n",
            "3242    A rational decision making framework for inhib...\n",
            "2415             Random Projections for Manifold Learning\n",
            "342     Statistically Efficient Estimations Using Cort...\n",
            "4764    The Bayesian Case Model: A Generative Approach...\n",
            "Name: title, dtype: object\n"
          ]
        }
      ]
    }
  ]
}