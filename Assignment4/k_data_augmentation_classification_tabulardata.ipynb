{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPkywflwt2vVxHeIvvFeYAX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ananya-AJ/CMPE255-Data-Mining/blob/main/Assignment4/k_data_augmentation_classification_tabulardata.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll use the UCI Credit Card Dataset. In this dataset, we are predicting whether or not a credit card client will default on their payment. The target variable is a binary variable, where 1 indicates that the client defaulted and 0 indicates that the client did not default"
      ],
      "metadata": {
        "id": "K1o01Sah5pAF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P8-VSzYmTtk-",
        "outputId": "47f738d4-df46-4916-f0f0-98f073e0c424"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-04-09 23:17:36--  https://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5539328 (5.3M) [application/x-httpd-php]\n",
            "Saving to: ‘default of credit card clients.xls’\n",
            "\n",
            "default of credit c 100%[===================>]   5.28M  9.34MB/s    in 0.6s    \n",
            "\n",
            "2023-04-09 23:17:37 (9.34 MB/s) - ‘default of credit card clients.xls’ saved [5539328/5539328]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_excel('default of credit card clients.xls', header=1)"
      ],
      "metadata": {
        "id": "imvIZR65Ty3h"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "preprocess the data by splitting it into a training and testing set, normalizing the features, and one-hot encoding the target variable"
      ],
      "metadata": {
        "id": "o7aNt7SxUA9m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Separate the features and target variable\n",
        "X = df.iloc[:, :-1].values\n",
        "y = df.iloc[:, -1].values\n",
        "\n",
        "# Split the data into a training and testing set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normalize the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# One-hot encode the target variable\n",
        "encoder = OneHotEncoder()\n",
        "y_train = encoder.fit_transform(y_train.reshape(-1, 1)).toarray()\n",
        "y_test = encoder.transform(y_test.reshape(-1, 1)).toarray()"
      ],
      "metadata": {
        "id": "iMHNz92GT9Jl"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before Augmentation"
      ],
      "metadata": {
        "id": "bLGq2W7nUWXC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    keras.layers.Dense(32, activation='relu'),\n",
        "    keras.layers.Dense(y_train.shape[1], activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUeBrNHmUEr4",
        "outputId": "7ae860f4-7b4d-4b22-ae09-4cf60ca56d08"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "750/750 [==============================] - 5s 5ms/step - loss: 0.4693 - accuracy: 0.8068 - val_loss: 0.4474 - val_accuracy: 0.8147\n",
            "Epoch 2/10\n",
            "750/750 [==============================] - 5s 7ms/step - loss: 0.4415 - accuracy: 0.8181 - val_loss: 0.4452 - val_accuracy: 0.8170\n",
            "Epoch 3/10\n",
            "750/750 [==============================] - 3s 3ms/step - loss: 0.4353 - accuracy: 0.8200 - val_loss: 0.4380 - val_accuracy: 0.8205\n",
            "Epoch 4/10\n",
            "750/750 [==============================] - 2s 2ms/step - loss: 0.4317 - accuracy: 0.8207 - val_loss: 0.4390 - val_accuracy: 0.8177\n",
            "Epoch 5/10\n",
            "750/750 [==============================] - 2s 2ms/step - loss: 0.4282 - accuracy: 0.8213 - val_loss: 0.4382 - val_accuracy: 0.8195\n",
            "Epoch 6/10\n",
            "750/750 [==============================] - 2s 2ms/step - loss: 0.4270 - accuracy: 0.8207 - val_loss: 0.4388 - val_accuracy: 0.8175\n",
            "Epoch 7/10\n",
            "750/750 [==============================] - 2s 3ms/step - loss: 0.4237 - accuracy: 0.8234 - val_loss: 0.4372 - val_accuracy: 0.8170\n",
            "Epoch 8/10\n",
            "750/750 [==============================] - 2s 3ms/step - loss: 0.4233 - accuracy: 0.8219 - val_loss: 0.4344 - val_accuracy: 0.8188\n",
            "Epoch 9/10\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.4217 - accuracy: 0.8232 - val_loss: 0.4363 - val_accuracy: 0.8170\n",
            "Epoch 10/10\n",
            "750/750 [==============================] - 2s 3ms/step - loss: 0.4203 - accuracy: 0.8221 - val_loss: 0.4356 - val_accuracy: 0.8150\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fefaefab400>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(\"Test accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQydmr_6UQ2A",
        "outputId": "c4081bae-225a-400d-af7d-5ebebed93d4e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "188/188 [==============================] - 0s 2ms/step - loss: 0.4356 - accuracy: 0.8150\n",
            "Test accuracy: 0.8149999976158142\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data augmentation: let's use noise injection as a data augmentation technique on the credit card dataset.\n",
        "\n",
        "One way to add noise to the data is by adding random Gaussian noise to each feature with a given mean and sd"
      ],
      "metadata": {
        "id": "y7SBel7BUZkX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def add_noise(feature, mean, std):\n",
        "    noise = np.random.normal(mean, std, len(feature))\n",
        "    return feature + noise\n",
        "\n",
        "# Add noise to each feature of the dataset\n",
        "for i in range(1, len(df.columns)):\n",
        "    df.iloc[:, i] = add_noise(df.iloc[:, i], 0, 0.1)"
      ],
      "metadata": {
        "id": "JSmSeK9IUIbt"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "# Separate features and labels\n",
        "X = df.iloc[:, :-1]\n",
        "y = df.iloc[:, -1]\n",
        "\n",
        "# Split the data into a training and testing set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normalize the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "J6EoGeaGVYnR"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "# Define the neural network model\n",
        "model = Sequential()\n",
        "model.add(Dense(64, activation='relu', input_dim=X_train.shape[1]))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=2)\n",
        "\n",
        "# Evaluate the model on the testing set\n",
        "score = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Test accuracy:\", score[1])"
      ],
      "metadata": {
        "id": "UVpOjzvwWMW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Added random Gaussian noise with mean 0 and standard deviation 0.1 to each feature of the dataset, except for the target variable.\n",
        "\n",
        "we infer that adding noise to the data can help in making the model more robust to noise in the input data, but it may not always improve the performance of the model. It's always a good idea to try different data augmentation techniques and see which ones work best for your particular problem.\n",
        "\n",
        "It reduces loss but worsens test accuracy"
      ],
      "metadata": {
        "id": "D411RoIBXjSM"
      }
    }
  ]
}